{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IEEE Fraud Detection - Notebook de suivi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La détection des fraudes en ligne est l'une des problématiques les plus courantes et sensibles dans de nombreux secteurs, en particulier les banques. Au cours des dernières années, les tentatives de fraude ont connu une forte hausse, ce qui rend la lutte contre ce phénomène très importante. \n",
    "\n",
    "Cette compétition est un problème de classification binaire - c'est-à-dire que notre variable cible est un attribut binaire (l'utilisateur fait-il le clic frauduleux ou non?)\n",
    "\n",
    "Et notre objectif est de classer les utilisateurs en \"frauduleux\" ou \"non frauduleux\" le mieux possible.\n",
    "\n",
    "On cherche à prédire la probabilité qu'une transaction en ligne soit frauduleuse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Séance 1\n",
    "\n",
    "Pendant cette première semaine, nous nous intéressons essentiellement à l'exploration des données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus précisément, nous analysons les dataframes train_identity, train_transaction, test_identity et test_transaction.\n",
    "\n",
    "On rassemble les données train et test via la variable TransactionID.\n",
    "\n",
    "La taille du dataset est pour les données ***train***, de 590540 observations et 434 variables.\n",
    "\n",
    "Et pour les données ***test*** : 506691 observations et  433 variables.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ci-dessous, les 5 premières lignes de l'échantillon train obtenu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dataset.png\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque qu'il semble y avoir beaucoup de données manquantes.\n",
    "\n",
    "On essaie ensuite de comprendre ce que chaque variable représente. En s'aidant du site Kaggle, on a réussi à comprendre certaines variables mais pas toutes.\n",
    "\n",
    "En effet, \n",
    "\n",
    "Il y a des données qualitatives discrètes :\n",
    "\n",
    " - ***ProductCD*** : code produit pour chaque transaction\n",
    " - ***card1-card6*** : Informations sur la carte de paiement\n",
    " - ***P_emaildomain, R_emaildomain***, domaine de messagerie de l'acheteur et du destinataire \n",
    " - ***addr1*** : région de facturation, ***addr2***: pays de facturation\n",
    " - ***M1-M9*** : Correspondance, comme les noms sur la carte et l'adresse, etc.\n",
    " - ***id_12, id_15, id_16, id_23, id_27, id_28, id_29, id_30, id_31, id_33, id_34, id_35, id_36, id_37, id_38***\n",
    " - ***DeviceType, DeviceInfo***  \n",
    " \n",
    "Le reste des variables sont des variables numériques : \n",
    "\n",
    " - ***TransactionDT***: timedelta à partir d'une datetime de référence donnée (pas un horodatage réel)\n",
    " \n",
    " - ***TransactionAMT***: c'est le montant de la transaction en USD\n",
    "\n",
    " - ***dist***: distance entre (sans limitation) l'adresse de facturation, l'adresse Nous avons quelques informations sur certaines variables :\n",
    "\n",
    " - ***TransactionDT***: timedelta à partir d'une datetime de référence donnée (pas un horodatage réel)\n",
    " \n",
    " - ***TransactionAMT***: c'est le montant de la transaction en USD\n",
    "\n",
    " - ***C1-C14***: comptage, comme le nombre d'adresses associées à la carte de paiement, etc. La signification réelle est masquée. \n",
    " \n",
    " - ***D1-D15***: Timedelta, comme les jours entre la transaction précédente, etc. \n",
    " \n",
    " - ***Vxxx***: Vesta a conçu de riches fonctionnalités, notamment le classement, le comptage et d'autres relations d'entité.\n",
    " \n",
    " - ***id01-id11***: Fonctionnalités numériques pour l'identité\n",
    " \n",
    " - ***IsFraud*** = 1 signifie transaction frauduleuse, sinon ***IsFraud*** = 0, la transaction est non-frauduleuse\n",
    "\n",
    "Enfin nous réfléchissons à la manière de procéder afin de gérer les valeurs manquantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pour la séance prochaine**\n",
    "\n",
    "Le but sera de trouver une façon de gérer les valeurs manquantes et de continuer la visualisation des données de manière plus précise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Séance 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Valeurs manquantes** :\n",
    " \n",
    " Comme mentionné précédemment, les données contiennent beaucoup de valeurs manquantes. \n",
    "\n",
    "<img src=\"nan1.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "On peut voir que $45\\%$ des données du train sont des valeurs manquantes\n",
    "\n",
    "<img src=\"nan2.png\" style=\"width: 200px;\"/>\n",
    "\n",
    "De plus, ce tableau nous indique que par exemple, la variable $id\\_24$ contient $99.20\\%$ de valeurs manquantes, ce qui est énorme! \n",
    "\n",
    "Nous décidons alors de supprimer les 74 variables contenant plus de 80% de valeurs manquantes. En effet, nous avons hésité entre supprimer les variables contenant plus de 90%, plus de 80% ou plus de 70% de valeurs manquantes.\n",
    "\n",
    "Mais supprimer les variables contenant plus de 90% valeurs manquantes ne nous permet pas de supprimer suffisamment de variables (12) et nous avons jugé qu'une variable contenant en 80 et 90% de valeurs manquante et une variable inutile.\n",
    "\n",
    "En revanche, en supprimant les variables contenant plus de 70% valeurs manquantes, on supprime beaucoup de variables (208) qui pourraient être importantes.\n",
    "\n",
    "\n",
    " - **Visualisation de certaines variables** : \n",
    " \n",
    "     - IsFraud : \n",
    "     \n",
    "<img src=\"fraud.png\" style=\"width: 500px;\"/>\n",
    " \n",
    "On observe que la plupart des transactions sont non frauduleuses. Si on utilise cette base de données comme base pour nos modèles prédictifs et nos analyses, nous pourrions obtenir beaucoup d'erreurs et nos algorithmes seront probablement trop adaptés car ils \"supposeront\" que la plupart des transactions ne sont pas de la fraude. Mais on ne veut pas que notre modèle suppose, nous voulons que notre modèle détecte les modèles qui donnent des signes de fraude !\n",
    "\n",
    "    - card 1 - card 6\n",
    "\n",
    "Ces variables nous donnent des informations sur les cartes de paiement.\n",
    "\n",
    "\n",
    "<img src=\"card_head.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "Les variables card4 et card6 sont des chaînes de caractères et ce sont les seules variables compréhensibles.\n",
    "\n",
    "<img src=\"card.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "On observe que dans le cas des fraudes et des non-fraudes, les cartes visa sont les plus utilisées. \n",
    "\n",
    "De plus, dans le cas des fraudes, le nombre de cartes de débit et de crédit sont à peu près les mêmes, contrairement au cas des non-fraudes où le nombre de cartes de débit est bien supérieur au nombre de cartes de crédit.\n",
    "\n",
    "     - TransactionAMt : \n",
    "\n",
    "Cette variable décrit le montant de la transaction.\n",
    "\n",
    "<img src=\"transaction.png\" style=\"width: 700px;\"/>\n",
    "\n",
    "On représente la distribution du log de la variable \"Transaction Amt\" pour les transactions frauduleuses et pour les transactions non frauduleuses. Ces distributions ont l'allure d'une loi normale et ont des variances comparables.\n",
    "On observe que la moyenne $\\mu_1$ du montant des transactions dans le groupe des fraudes est différente de la moyenne $\\mu_2$ dans le cas des transactions non frauduleuses.\n",
    "\n",
    "On fait alors un test de student, pour vérifier cette observation.\n",
    "\n",
    "On teste, $H_0 : \\mu_1 = \\mu_2$ contre $H_1 : \\mu_1 \\neq \\mu_2$\n",
    "\n",
    "Ainsi, on trouve une $p-value = 3 \\times 10^{-19}<0.05$.\n",
    "\n",
    "Donc on rejette $H_0$, c'est-à-dire que les moyennes des deux distributions ne sont effectivement pas égales. \n",
    "\n",
    "        - TransactionDT :\n",
    "        \n",
    "On trace la variable \"TransactionDT\" pour l'échantillon train et test. \n",
    "\n",
    "<img src=\"transDT.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "On remarque que les dates de transactions de train et de test ne se chevauchent pas, il serait donc prudent d'utiliser la répartition temporelle pour la validation.\n",
    "\n",
    " - **Github** :\n",
    "\n",
    "Nous apprenons également à utiliser github étant donné qu'aucun d'entre nous n'est familier avec cette platform.\n",
    "\n",
    " - **Premier modèle** :\n",
    " \n",
    "Implémentation de l'algorithme Random Forest sur un nombre resteint de données: accuracy= 0,8.\n",
    "On obtient une grande valeur d'accuracy, néanmoins, cette metric n'est pas appropriée lorsque les classes ne sont pas équilibrée.\n",
    "En effet, comme on a remarqué précédemmment, la distribution de IsFraud n'est pas du tout équilibrée (il y a beaucoup plus de 0 que de 1), et donc même si le modèle prédit 0 tout le temps, cela donnera une grande accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pour la séance prochaine**\n",
    "\n",
    "Nous nous sommes répartis le travail comme ceci : \n",
    "\n",
    "- Réfléchir à une méthode pour gérer les variables contenant des chaînes de caractères (Auxence)\n",
    "- Regarder les correlations entre les variables (Anthony)\n",
    "- Faire des modèles simples sur un petit nombre de variables, par exemple Naive Bayes, LDA (Amina)\n",
    "- Commencer à rédiger le rapport (Yamina)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Séance 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Rédaction du début du rapport**\n",
    " \n",
    "Lors de cette séance, nous entamons la rédaction du rapport (introduction, début d'exploration des données).\n",
    "\n",
    " - **Application de l'algorithme Naive Bayes**\n",
    "\n",
    "On commence par appliquer le classifier Naive Bayes car son utilisation ne nécessite pas l'ajustement d'hyperparamètres.\n",
    "\n",
    "La classification naïve bayésienne est un type de classification bayésienne probabiliste simple basée sur le théorème de Bayes avec une forte indépendance des hypothèses.\n",
    "On peut alors prédire la valeur de la variable à expliquer, ici il s'agit de IsFraud, à l'aide de la formule suivante.\n",
    "\n",
    "<img src=\"naive.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "où $y$ : IsFraud qui vaut 0 ou 1\n",
    "\n",
    "et $x_1,...,x_n$ sont les variables explicatives.\n",
    "\n",
    "Avantages et inconvénients de Naive Bayes:\n",
    "\n",
    "Avantages\n",
    "\n",
    " - C’est relativement simple à comprendre et à construire\n",
    " - Il est facile à former, même avec un petit jeu de données\n",
    " - C’est rapide!\n",
    " - Il n’est pas sensible aux caractéristiques non pertinentes\n",
    "\n",
    "Désavantages\n",
    "\n",
    " - Il implique que chaque fonctionnalité soit indépendante, ce qui n’est pas toujours le cas.\n",
    "\n",
    "\n",
    " On applique cet algorithme en ne prenant en compte que les variables continues. On remplace les NA par la médiane car c'est une méthode généralement utilisée. \n",
    " \n",
    "On trouve alors : AUC = 0.58, ce qui n'est pas très bon.\n",
    "\n",
    "Afin de pouvoir juger de l'efficacité d'un modèle, On utilise l'AUC qui correspond à l'aire sous la courbe ROC. \n",
    "\n",
    "Une courbe ROC (receiver operating characteristic) est un graphique représentant les performances d'un modèle de classification pour tous les seuils de classification. Cette courbe trace le taux de vrais positifs en fonction du taux de faux positifs.\n",
    "\n",
    "<img src=\"roc.png\" style=\"width: 300px;\"/>\n",
    " \n",
    " - **Récupération des variables indépendantes de isFraud et application d'une régression logistique**\n",
    " \n",
    "Elle décrit la modélisation d’une variable qualitative Y à 2 modalités : 1 ou 0. Dans notre cas Y correspond à IsFraud.\n",
    "\n",
    "La régression logistique cherche à estimer la probabilité\n",
    "$P(Y|X)$ : \n",
    "Y a deux modalités, donc :\n",
    "\n",
    "$\\frac{P(Y=1|X)}{P(Y=0|X)}$ = $\\frac{P(Y=1)*P(X|Y=1)}{P(Y=0)*P(X|Y=0)}$\n",
    "\n",
    "Dans un premier temps, nous utilisons la régression logistique avec les paramètres par défaut.\n",
    "\n",
    "On trouve un AUC = 0.809\n",
    "\n",
    " - **Convertir les variables contenant des chaînes de caractère en numérique** :\n",
    " \n",
    " Nous utilisons la méthode de label encoding qui consiste à remplacer par des chiffres les chaînes de caractères.\n",
    " Par exemple, la variable card-6 prend pour valeur credit et debit, on les remplace alors par credit=1 et debit=2.\n",
    " \n",
    " - Encodage des chaînes de caractères en utilisant la méthode one hot encoder (Auxence) :\n",
    " \n",
    "Nous nous sommes rendus compte que la méthode de label encoding n'est pas appropriée, car elle suggère qu'il y a un ordre entre les valeurs, en utilisant l'exemple précédent, on aurait debit=2>credit=1, ce qui n'a pas de sens.\n",
    "\n",
    "Nous utilisons alors l'encodage one-hot. \n",
    "\n",
    "Cette méthode consiste à créer et ajouter des colonnes binaires qui réfèrent ou non la donnée par un 0 ou 1.\n",
    "\n",
    "Par exemple, la variable 'card4' peut prendre les valeurs visa, american express, discover ou mastercard. Ainsi, avec l'encodage one-hot, quatres colonnes  card4\\_visa,card4\\_americanexpress, card4\\_discover et card4\\_mastercard sont créees où si la transaction à été faite par carte visa par exemple, la valeur de la variable card4\\_visa est de 1 et celle des autres variables est de 0, comme représenté sur la photo ci-dessous:\n",
    "\n",
    "<img src=\"card4.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pour la semaine prochaine**\n",
    "\n",
    "Nous nous sommes répartis le travail : \n",
    " \n",
    "- Finir l'exploration des données en utilisant des méthodes de régularisation (Lasso, Ridge)(Anthony)\n",
    "- Mettre en commun tous les notebooks (Amina)\n",
    "- Appliquer d'autres algorithmes de machine learning en prenant en compte toutes les variables (Amina)\n",
    "- Continuer la rédaction du rapport (Yamina)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Séance 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pendant cette séance, nous nous intéressons à appliquer des algorithmes de Machine Learning à nos données pré-traitées.\n",
    "\n",
    "- **Tester cross validation pour le paramètre inverse of regularization strength pour la méthode de régression logistique**\n",
    "\n",
    "La régression logistique consiste à faire l'hypothèse que cette quantité peut être écrite à l'aide d'une fonction linéaire en X et par la suite à maximiser la log-vraisemblance. Toutefois, afin d'éviter un overfitting ou un problème de convergence aux algorithmes qui approximent les coefficients de régression, on peut pénaliser les grands coefficients. Cela peut se faire en ajoutant un terme de pénalisation à la fonction de log-vraisemblance. \n",
    "\n",
    "La fonction que nous avons utilisé nous a permis d'utiliser 3 types de pénalisation. La régularisation L1, la régularisarion L2 et elasticnet qui est une combinaison des deux premières. De plus, la régularisation L1 est aussi utile pour de la selection de variable.\n",
    "\n",
    "Si on considère un dataset de n lignes et p colonnes.\n",
    "\n",
    "La régularisation L1 (Lasso) : \n",
    "\n",
    "La regularisation L1 pénalise les coefficients $|\\beta_j|$\n",
    "\n",
    "<img src=\"lasso.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "La régularisation L2 (Ridge) : \n",
    "\n",
    "La regularisation L2 pénalise les coefficients $\\beta_j^2$\n",
    "\n",
    "<img src=\"ridge.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n",
    " - **Lasso pour sélection de variables** : \n",
    " \n",
    "Pour $\\lambda$=1 et pour $\\lambda$=0.1 nous observons que toutes les variables ont un coefficient nul.\n",
    "\n",
    "Et pour $\\lambda$=0.01, seulement 20 variables ont un coefficient non nul, ce qui est trop peu.\n",
    "\n",
    "Nous décidons alors de ne pas utiliser cette méthode pour faire une sélection de variables. \n",
    "\n",
    "\n",
    "\n",
    "- **Commencer à regarder l'algorithme Adaptive Boosting**\n",
    "\n",
    "Adaboost combine plusieurs apprenants faibles en un seul apprenant fort. \n",
    "\n",
    "Les apprenants faibles dans AdaBoost sont des arbres de décision avec une seule division, appelés souches de décision. \n",
    "\n",
    "Lorsque AdaBoost crée son premier noeud de décision, toutes les observations sont pondérées de manière égale. Pour corriger l'erreur précédente, les observations incorrectement classées ont désormais plus de poids que les observations correctement classées. \n",
    "\n",
    "Les algorithmes AdaBoost peuvent être utilisés pour les problèmes de classification.\n",
    "\n",
    "\n",
    "- **Voir la méthode K-NN**\n",
    "\n",
    "On utilise la méthode des k plus proches voisins (K-NN).\n",
    "Pour effectuer une prédiction, l’algorithme K-NN va se baser sur le jeu de données en entier. En effet, pour une observation, qui ne fait pas parti du jeu de données, qu’on souhaite prédire, l’algorithme va chercher les K instances du jeu de données les plus proches de notre observation. Ensuite pour ces K voisins, l’algorithme se basera sur leurs variables de sortie IsFraud pour calculer la valeur de la variable IsFraud de l’observation qu’on souhaite prédire. \n",
    "\n",
    "Dans le cadre de la classification, c’est le mode des variables IsFraud des K plus proches observations qui servira pour la prédiction.\n",
    "\n",
    "Le paramètre important de cette méthode est le nombre de voisins considérés (n_neighbors).\n",
    "on utilise gridsearch pour tester toutes les valeurs de n_neighbors.\n",
    "\n",
    "\n",
    "- **Application du modèle random forest**\n",
    "\n",
    "Random Forest est un modèle composé de nombreux arbres de décision. Plutôt que de simplement faire la moyenne de la prédiction des arbres (que nous pourrions appeler une «forêt»), ce modèle utilise deux concepts clés qui lui donnent le nom aléatoire:\n",
    "\n",
    "- Échantillonnage aléatoire des points de données d'entraînement lors de la construction d'arbres\n",
    "\n",
    "- Sous-ensembles aléatoires de variables pris en compte lors de la division des nœuds\n",
    "\n",
    "**Avantages:**\n",
    "\n",
    "- L'algorithme _Random Forest_ est considéré comme une méthode très précise et robuste en raison du nombre d'arbres de décision participant au processus.\n",
    "\n",
    "- Rapide dans les calculs\n",
    "\n",
    "- Classe les variables explicatives en fonction de leur lien avec les variables à expliquer \n",
    "\n",
    "- Il ne souffre pas du problème de sur-ajustement. La raison principale est qu'il prend la moyenne de toutes les prédictions, ce qui annule les biais.\n",
    "\n",
    "- On peut obtenir l'importance relative des variables, ce qui aide à sélectionner les variables les plus contributives pour le classificateur.\n",
    "\n",
    "**Paramètres:**\n",
    "\n",
    "- _n_estimators:_ Nombre d'arbres dans la forêt. La valeur par défaut est 10.\n",
    "\n",
    "- _min_samples_split:_ nombre minimum de  nœud requis pour la division. La valeur par défaut est 2.\n",
    "\n",
    "- _max_features:_  nombre de variables à considérer lors de la recherche de la meilleure répartition. si 'sqrt' alors max_features = sqrt (n_features)\n",
    "\n",
    "- _n_jobs_= -1 signifie utiliser tous les processeurs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pour la séance prochaine**\n",
    "\n",
    "Nous nous sommes répartis le travail comme ceci :\n",
    "- Continuer à travailler sur le modèle random forest (Yamina)\n",
    "- Contrinuer à travailler sur la cross validation pour le paramètre inverse of regularization strength pour la méthode de régression logistique (Anthony)\n",
    "- Continuer à travailler sur la méthode KNN (Amina)\n",
    "- Continuer à travailler sur la méthode Adaboost(Auxence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Séance 5\n",
    "\n",
    "**Random forest :**\n",
    "\n",
    "- Entrainement des données avec les paramètres suivants: n_jobs=-1, n_estimators = 40,max_features = 'sqrt' puis prédiction sur les données tests,\n",
    "- Recherche du nombre moyen des noeuds et la profondeur maximale moyenne pour chaque arbre,\n",
    "- courbe ROC et résultat AUC sur l'ensemble de variables,\n",
    "- Réglage d'hyperparamètres: on a essayé deux méthodes (1) On a effectuer une recherche aléatoire de meilleurs hyperparamètres. Cela sélectionnera de manière aléatoire des combinaisons d'hyperparamètres à partir d'une grille, les évaluera à l'aide d'une validation croisée sur les données d'entraînement et renverra les valeurs les plus performantes. (2)On s'est concentré sur 3 hyperparamètres.\n",
    "- Selection des variables importantes\n",
    "- Reprendre le modèle avec les variables les plus importantes et les hyperparamètres optimaux.\n",
    "- L'option (2) Retourne un meilleur résultat AUC qui est passé de 0.92 à 0.94\n",
    "\n",
    "**Régression logistique :**\n",
    "\n",
    "\n",
    "\n",
    "**Knn :**\n",
    "\n",
    "**Adaboost :**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
