{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semaine 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration des données:\n",
    "- Analyse des dataframes train_identity, train_transaction, test_identity et test_transaction\n",
    "- Jointure des dataframes\n",
    "- Compréhension des données: Il s'agit de savoir ce que chaque colonne répresente, s'il s'agit d'une variable numérique ou catégorielle.\n",
    "- Le traitement des valeurs manquantes dans les dataframe: Nous réflichissons sur comment traiter les NaN de chaque variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semaine 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suite exploration des données:\n",
    "- suppression des variables contenant plus de 80% de NAN\n",
    "- visualisation des données à l'aide de graphiques: On a remarqué que la plupart des transactions sont non frauduleuses. Si on utilise cette base de données comme base pour nos modèles prédictifs et nos analyses, nous pourrions obtenir beaucoup d'erreurs et nos algorithmes seront probablement trop adaptés car ils \"supposeront\" que la plupart des transactions ne sont pas de la fraude. Mais on ne veut pas que notre modèle suppose, nous voulons que notre modèle détecte les modèles qui donnent des signes de fraude!\n",
    "- test de student, comparaison du montant de transaction \"amt\" moyen pour les transaction frauduleuses et non frauduleuses, On a trouvé une p-value largement inférieure au seuil(5%), on rejette alors notre test, donc le montant de transaction \"amt\" moyen pour les transactions frauduleuses est significativement différents du montant de transaction moyen pour les transaction non frauduleuses \n",
    "- Tracer de la variable \"TransacrionDT\" pour l'échantillon train et test. On a remarqué que les dates de transactions de train et de test ne se chevauchent pas, il serait donc prudent d'utiliser la répartition temporelle pour la validation\n",
    "- Apprendre à utiliser github \n",
    "- Implémentation de l'algorithme Random Forest sur un nombre resteint de données: accuracy= 0,8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pour la semaine prochaine**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gérer les strings (auxence)\n",
    "- regarder les correlations entre les variables (anthony)\n",
    "- faire des modèles simples sur un petit nombre de variables, par exemple Naive Bayes, LDA (amina)\n",
    "- commencer à rédiger le rapport (yamina)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semaine 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Rédaction du début du rapport (introduction, début d'exploration des données)\n",
    " - Application de modèles simples (Naive Bayes, LDA) en ne prenant en compte que les variables continues et en remplaçant les NA par la médiane, AUC = 0.58\n",
    " - Faire une soumission Kaggle\n",
    " - Récupération des variables indépendantes de isFraud et application d'une régression logistique\n",
    " - Convertir les variables contenant des chaînes de caractère en numérique \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pour la semaine prochaine**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encodage des chaînes de caractères (Auxence)\n",
    "- Screening (pré sélection)(Auxence)\n",
    "- Finir l'exploration des données (régularisation)(Anthony)\n",
    "- Mettre en commun tous les notebooks (Amina)\n",
    "- Appliquer d'autres algorithmes de machine learning en prenant en compte toutes les variables (Amina)\n",
    "- Continuer la rédaction du rapport (Yamina)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semaine 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tester cross validation pour le paramètre inverse of regularization strength pour la méthode de régression logistique\n",
    "- Commencer à regarder l'algorithme gradient boostrap\n",
    "- Voir la méthode KNN (théorie et pratique)\n",
    "- Application du modèle Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pour la semaine prochaine**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Continuer à travailler sur le modèle gradient boostrap (Yamina)\n",
    "- Contrinuer à travailler sur la cross validation pour le paramètre inverse of regularization strength pour la méthode de régression logistique (Anthony)\n",
    "- Continuer à travailler sur la méthode KNN (Auxence)\n",
    "- Mettre en commun tous les notebooks et continuer à travailler sur d'autres algorithmes de ML (Amina)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- modèle gradient boostrap remplacé par le modèle random forest (yamina)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
